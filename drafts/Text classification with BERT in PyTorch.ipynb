{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"data/text_classification/20newsgroups_train.tsv\"\n",
    "DEV_PATH = \"data/text_classification/20newsgroups_dev.tsv\"\n",
    "TEST_PATH = \"data/text_classification/20newsgroups_test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train = fetch_20newsgroups(subset=\"train\")\n",
    "target_names = train.target_names\n",
    "label2idx = {label: idx for idx, label in enumerate(target_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "from torchtext.data import TabularDataset, Field, BucketIterator\n",
    "\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "text = Field(sequential=False)\n",
    "label = Field(sequential=False, use_vocab=False)\n",
    "\n",
    "train_data = TabularDataset(path=TRAIN_PATH, format='tsv', fields=[('label', label), ('text', text)])\n",
    "dev_data = TabularDataset(path=DEV_PATH, format='tsv', fields=[('label', label), ('text', text)])\n",
    "test_data = TabularDataset(path=TEST_PATH, format='tsv', fields=[('label', label), ('text', text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-pretrained-bert\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/68/84de54aea460eb5b2e90bf47a429aacc1ce97ff052ec40874ea38ae2331d/pytorch_pretrained_bert-0.4.0-py3-none-any.whl (45kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 1.4MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: boto3 in /Users/yvespeirsman/anaconda3/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.7.6)\n",
      "Requirement already satisfied: requests in /Users/yvespeirsman/anaconda3/lib/python3.6/site-packages (from pytorch-pretrained-bert) (2.18.4)\n",
      "Requirement already satisfied: numpy in /Users/yvespeirsman/anaconda3/lib/python3.6/site-packages (from pytorch-pretrained-bert) (1.15.4)\n",
      "Requirement already satisfied: torch>=0.4.1 in /Users/yvespeirsman/anaconda3/lib/python3.6/site-packages (from pytorch-pretrained-bert) (0.4.1)\n",
      "Requirement already satisfied: tqdm in /Users/yvespeirsman/anaconda3/lib/python3.6/site-packages (from pytorch-pretrained-bert) (4.19.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/yvespeirsman/anaconda3/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.9.3)\n",
      "Requirement already satisfied: botocore<1.11.0,>=1.10.6 in /Users/yvespeirsman/anaconda3/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (1.10.6)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /Users/yvespeirsman/anaconda3/lib/python3.6/site-packages (from boto3->pytorch-pretrained-bert) (0.1.13)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/yvespeirsman/anaconda3/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /Users/yvespeirsman/anaconda3/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /Users/yvespeirsman/anaconda3/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yvespeirsman/anaconda3/lib/python3.6/site-packages (from requests->pytorch-pretrained-bert) (2018.10.15)\n",
      "Requirement already satisfied: docutils>=0.10 in /Users/yvespeirsman/anaconda3/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.6->boto3->pytorch-pretrained-bert) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /Users/yvespeirsman/anaconda3/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.6->boto3->pytorch-pretrained-bert) (2.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/yvespeirsman/anaconda3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.11.0,>=1.10.6->boto3->pytorch-pretrained-bert) (1.11.0)\n",
      "Installing collected packages: pytorch-pretrained-bert\n",
      "Successfully installed pytorch-pretrained-bert-0.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-pretrained-bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2idx = {\"pos\": 1, \"neg\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available models are `bert-base-uncased`, `bert-large-uncased`, `bert-base-cased`, `bert-base-multilingual` and `bert-base-chinese`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "BERT_MODEL = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/407873900 [00:00<?, ?B/s]\u001b[A\n",
      "  0%|          | 17408/407873900 [00:00<46:48, 145198.06B/s]\u001b[A\n",
      "  0%|          | 52224/407873900 [00:00<39:57, 170117.51B/s]\u001b[A\n",
      "  0%|          | 139264/407873900 [00:00<30:51, 220272.48B/s]\u001b[A\n",
      "  0%|          | 330752/407873900 [00:00<22:52, 296850.24B/s]\u001b[A\n",
      "  0%|          | 591872/407873900 [00:00<17:01, 398831.09B/s]\u001b[A\n",
      "  0%|          | 852992/407873900 [00:00<12:46, 531294.41B/s]\u001b[A\n",
      "  0%|          | 1618944/407873900 [00:00<09:14, 733057.03B/s]\u001b[A\n",
      "  1%|          | 2658304/407873900 [00:00<06:40, 1010904.11B/s]\u001b[A\n",
      "  1%|          | 3303424/407873900 [00:01<04:58, 1353262.90B/s]\u001b[A\n",
      "  1%|          | 3954688/407873900 [00:01<03:47, 1775114.13B/s]\u001b[A\n",
      "  1%|          | 4804608/407873900 [00:01<02:53, 2323989.77B/s]\u001b[A\n",
      "  1%|▏         | 5447680/407873900 [00:01<02:27, 2726803.19B/s]\u001b[A\n",
      "  2%|▏         | 6164480/407873900 [00:01<02:03, 3252820.94B/s]\u001b[A\n",
      "  2%|▏         | 7196672/407873900 [00:01<01:40, 3991109.36B/s]\u001b[A\n",
      "  2%|▏         | 7863296/407873900 [00:01<01:28, 4495731.55B/s]\u001b[A\n",
      "  2%|▏         | 8671232/407873900 [00:01<01:22, 4815164.06B/s]\u001b[A\n",
      "  2%|▏         | 9703424/407873900 [00:02<01:11, 5538940.22B/s]\u001b[A\n",
      "  3%|▎         | 10506240/407873900 [00:02<01:05, 6086129.51B/s]\u001b[A\n",
      "  3%|▎         | 11233280/407873900 [00:02<01:03, 6231601.22B/s]\u001b[A\n",
      "  3%|▎         | 12015616/407873900 [00:02<00:59, 6615832.00B/s]\u001b[A\n",
      "  3%|▎         | 12742656/407873900 [00:02<01:02, 6314276.69B/s]\u001b[A\n",
      "  3%|▎         | 13422592/407873900 [00:02<01:03, 6174450.67B/s]\u001b[A\n",
      "  3%|▎         | 14241792/407873900 [00:02<00:59, 6664667.71B/s]\u001b[A\n",
      "  4%|▎         | 14944256/407873900 [00:02<00:58, 6683029.02B/s]\u001b[A\n",
      "  4%|▍         | 15637504/407873900 [00:02<01:01, 6342803.09B/s]\u001b[A\n",
      "  4%|▍         | 16444416/407873900 [00:03<00:57, 6777324.34B/s]\u001b[A\n",
      "  4%|▍         | 17145856/407873900 [00:03<00:57, 6830221.09B/s]\u001b[A\n",
      "  4%|▍         | 17847296/407873900 [00:03<00:56, 6883723.37B/s]\u001b[A\n",
      "  5%|▍         | 18681856/407873900 [00:03<00:53, 7215455.88B/s]\u001b[A\n",
      "  5%|▍         | 19435520/407873900 [00:03<00:53, 7231101.44B/s]\u001b[A\n",
      "  5%|▍         | 20287488/407873900 [00:03<00:51, 7530992.39B/s]\u001b[A\n",
      "  5%|▌         | 21050368/407873900 [00:03<00:52, 7419797.96B/s]\u001b[A\n",
      "  5%|▌         | 21827584/407873900 [00:03<00:51, 7519067.95B/s]\u001b[A\n",
      "  6%|▌         | 22585344/407873900 [00:03<00:52, 7332985.32B/s]\u001b[A\n",
      "  6%|▌         | 23516160/407873900 [00:03<00:49, 7831458.45B/s]\u001b[A\n",
      "  6%|▌         | 24312832/407873900 [00:04<00:50, 7548112.07B/s]\u001b[A\n",
      "  6%|▌         | 25078784/407873900 [00:04<00:51, 7504473.69B/s]\u001b[A\n",
      "  6%|▋         | 25837568/407873900 [00:04<00:52, 7209264.81B/s]\u001b[A\n",
      "  7%|▋         | 26566656/407873900 [00:04<00:53, 7179725.51B/s]\u001b[A\n",
      "  7%|▋         | 27290624/407873900 [00:04<00:53, 7080219.40B/s]\u001b[A\n",
      "  7%|▋         | 28119040/407873900 [00:04<00:51, 7392381.96B/s]\u001b[A\n",
      "  7%|▋         | 28865536/407873900 [00:04<00:54, 6919854.82B/s]\u001b[A\n",
      "  7%|▋         | 29642752/407873900 [00:04<00:55, 6788960.51B/s]\u001b[A\n",
      "  7%|▋         | 30530560/407873900 [00:04<00:51, 7303702.04B/s]\u001b[A\n",
      "  8%|▊         | 31278080/407873900 [00:05<00:55, 6826122.20B/s]\u001b[A\n",
      " 43%|████▎     | 174301184/407873900 [00:40<00:34, 6701333.61B/s][A\n",
      "  8%|▊         | 32802816/407873900 [00:05<00:52, 7170563.86B/s]\u001b[A\n",
      "  8%|▊         | 33530880/407873900 [00:05<01:00, 6231279.12B/s]\u001b[A\n",
      "  8%|▊         | 34263040/407873900 [00:05<00:58, 6362264.65B/s]\u001b[A\n",
      "  9%|▊         | 35229696/407873900 [00:05<00:53, 6972136.57B/s]\u001b[A\n",
      "  9%|▉         | 35966976/407873900 [00:05<00:52, 7024761.49B/s]\u001b[A\n",
      "  9%|▉         | 36692992/407873900 [00:05<00:58, 6322350.72B/s]\u001b[A\n",
      "  9%|▉         | 37425152/407873900 [00:05<00:56, 6561477.38B/s]\u001b[A\n",
      "  9%|▉         | 38211584/407873900 [00:06<00:54, 6798043.64B/s]\u001b[A\n",
      " 10%|▉         | 38909952/407873900 [00:06<00:57, 6457170.20B/s]\u001b[A\n",
      " 10%|▉         | 39572480/407873900 [00:06<00:57, 6430467.07B/s]\u001b[A\n",
      " 10%|▉         | 40554496/407873900 [00:06<00:51, 7130389.75B/s]\u001b[A\n",
      " 10%|█         | 41299968/407873900 [00:06<00:52, 6981238.30B/s]\u001b[A\n",
      " 10%|█         | 42021888/407873900 [00:06<00:53, 6889821.10B/s]\u001b[A\n",
      " 11%|█         | 42946560/407873900 [00:06<00:50, 7210477.77B/s]\u001b[A\n",
      " 11%|█         | 43813888/407873900 [00:06<00:47, 7594407.56B/s]\u001b[A\n",
      " 11%|█         | 44590080/407873900 [00:06<00:52, 6932666.21B/s]\u001b[A\n",
      " 11%|█         | 45387776/407873900 [00:07<00:50, 7164639.42B/s]\u001b[A\n",
      " 11%|█▏        | 46192640/407873900 [00:07<00:48, 7408055.21B/s]\u001b[A\n",
      " 12%|█▏        | 46948352/407873900 [00:07<00:52, 6866335.20B/s]\u001b[A\n",
      " 12%|█▏        | 47653888/407873900 [00:07<00:53, 6690570.23B/s]\u001b[A\n",
      " 12%|█▏        | 48615424/407873900 [00:07<00:48, 7347518.69B/s]\u001b[A\n",
      " 12%|█▏        | 49379328/407873900 [00:07<00:51, 7007220.46B/s]\u001b[A\n",
      " 12%|█▏        | 50139136/407873900 [00:07<00:49, 7163159.03B/s]\u001b[A\n",
      " 12%|█▏        | 50873344/407873900 [00:07<00:51, 6919026.61B/s]\u001b[A\n",
      " 13%|█▎        | 51579904/407873900 [00:08<01:01, 5794785.86B/s]\u001b[A\n",
      " 13%|█▎        | 52514816/407873900 [00:08<00:55, 6429072.02B/s]\u001b[A\n",
      " 13%|█▎        | 53366784/407873900 [00:08<00:51, 6934792.21B/s]\u001b[A\n",
      " 13%|█▎        | 54107136/407873900 [00:08<00:53, 6663236.17B/s]\u001b[A\n",
      " 13%|█▎        | 54849536/407873900 [00:08<00:51, 6871486.45B/s]\u001b[A\n",
      " 14%|█▎        | 55692288/407873900 [00:08<00:48, 7274238.44B/s]\u001b[A\n",
      " 14%|█▍        | 56444928/407873900 [00:08<00:51, 6812025.99B/s]\u001b[A\n",
      " 14%|█▍        | 57181184/407873900 [00:08<00:50, 6968028.78B/s]\u001b[A\n",
      " 14%|█▍        | 58019840/407873900 [00:08<00:47, 7335206.23B/s]\u001b[A\n",
      " 14%|█▍        | 58771456/407873900 [00:09<00:54, 6439632.61B/s]\u001b[A\n",
      " 15%|█▍        | 59510784/407873900 [00:09<00:55, 6243280.85B/s]\u001b[A\n",
      " 15%|█▍        | 60503040/407873900 [00:09<00:49, 7008707.14B/s]\u001b[A\n",
      " 15%|█▌        | 61251584/407873900 [00:09<00:53, 6443403.83B/s]\u001b[A\n",
      " 15%|█▌        | 61938688/407873900 [00:09<01:00, 5764995.36B/s]\u001b[A\n",
      " 15%|█▌        | 62682112/407873900 [00:09<00:55, 6180648.95B/s]\u001b[A\n",
      " 16%|█▌        | 63475712/407873900 [00:09<00:52, 6618594.84B/s]\u001b[A\n",
      " 16%|█▌        | 64174080/407873900 [00:09<00:52, 6524293.81B/s]\u001b[A\n",
      " 16%|█▌        | 65015808/407873900 [00:09<00:49, 6987245.77B/s]\u001b[A\n",
      " 16%|█▌        | 65741824/407873900 [00:10<00:53, 6441910.42B/s]\u001b[A\n",
      " 16%|█▋        | 66703360/407873900 [00:10<00:48, 6981669.26B/s]\u001b[A\n",
      " 17%|█▋        | 67489792/407873900 [00:10<00:47, 7224895.31B/s]\u001b[A\n",
      " 17%|█▋        | 68237312/407873900 [00:10<00:48, 7028545.63B/s]\u001b[A\n",
      " 17%|█▋        | 68988928/407873900 [00:10<00:47, 7155142.43B/s]\u001b[A\n",
      " 17%|█▋        | 69718016/407873900 [00:10<00:47, 7160265.30B/s]\u001b[A\n",
      " 17%|█▋        | 70444032/407873900 [00:10<00:49, 6822702.78B/s]\u001b[A\n",
      " 17%|█▋        | 71211008/407873900 [00:10<00:47, 7055741.63B/s]\u001b[A\n",
      " 18%|█▊        | 71925760/407873900 [00:11<01:17, 4310283.42B/s]\u001b[A\n",
      " 18%|█▊        | 72683520/407873900 [00:11<01:09, 4788747.74B/s]\u001b[A\n",
      " 18%|█▊        | 73715712/407873900 [00:11<01:01, 5401461.39B/s]\u001b[A\n",
      " 18%|█▊        | 74747904/407873900 [00:11<00:54, 6091275.99B/s]\u001b[A\n",
      " 19%|█▊        | 75460608/407873900 [00:11<00:55, 5983631.41B/s]\u001b[A\n",
      " 19%|█▊        | 76131328/407873900 [00:11<00:56, 5831441.64B/s]\u001b[A\n",
      " 19%|█▉        | 77123584/407873900 [00:11<00:49, 6641479.36B/s]\u001b[A\n",
      " 19%|█▉        | 77910016/407873900 [00:11<00:47, 6957994.08B/s]\u001b[A\n",
      " 19%|█▉        | 78662656/407873900 [00:12<00:47, 6881782.83B/s]\u001b[A\n",
      " 19%|█▉        | 79390720/407873900 [00:12<00:49, 6628989.27B/s]\u001b[A\n",
      " 20%|█▉        | 80083968/407873900 [00:12<00:55, 5911892.19B/s]\u001b[A\n",
      " 20%|█▉        | 80859136/407873900 [00:12<00:52, 6219730.24B/s]\u001b[A\n",
      " 20%|██        | 81612800/407873900 [00:12<00:49, 6553708.49B/s]\u001b[A\n",
      " 20%|██        | 82366464/407873900 [00:12<00:52, 6144585.29B/s]\u001b[A\n",
      " 20%|██        | 83004416/407873900 [00:12<00:53, 6056464.71B/s]\u001b[A\n",
      " 21%|██        | 83759104/407873900 [00:12<00:51, 6299436.50B/s]\u001b[A\n",
      " 21%|██        | 84611072/407873900 [00:13<00:48, 6731235.31B/s]\u001b[A\n",
      " 21%|██        | 85374976/407873900 [00:13<00:46, 6979260.33B/s]\u001b[A\n",
      " 21%|██        | 86167552/407873900 [00:13<00:44, 7232352.45B/s]\u001b[A\n",
      " 21%|██▏       | 86937600/407873900 [00:13<00:43, 7348944.65B/s]\u001b[A\n",
      " 21%|██▏       | 87682048/407873900 [00:13<00:44, 7225707.84B/s]\u001b[A\n",
      " 22%|██▏       | 88412160/407873900 [00:13<00:47, 6749954.05B/s]\u001b[A\n",
      " 22%|██▏       | 89395200/407873900 [00:13<00:44, 7160424.89B/s]\u001b[A\n",
      " 22%|██▏       | 90427392/407873900 [00:13<00:41, 7579146.87B/s]\u001b[A\n",
      " 22%|██▏       | 91202560/407873900 [00:13<00:41, 7625249.39B/s]\u001b[A\n",
      " 23%|██▎       | 91977728/407873900 [00:14<00:45, 7015095.77B/s]\u001b[A\n",
      " 23%|██▎       | 92807168/407873900 [00:14<00:42, 7354327.31B/s]\u001b[A\n",
      " 23%|██▎       | 93673472/407873900 [00:14<00:40, 7702528.10B/s]\u001b[A\n",
      " 23%|██▎       | 94459904/407873900 [00:14<00:42, 7366936.98B/s]\u001b[A\n",
      " 23%|██▎       | 95211520/407873900 [00:14<00:44, 7032114.31B/s]\u001b[A\n",
      " 24%|██▎       | 96112640/407873900 [00:14<00:43, 7088447.35B/s]\u001b[A\n",
      " 24%|██▍       | 97112064/407873900 [00:14<00:40, 7745358.69B/s]\u001b[A\n",
      " 24%|██▍       | 97911808/407873900 [00:14<00:42, 7270842.51B/s]\u001b[A\n",
      " 24%|██▍       | 98663424/407873900 [00:14<00:42, 7342356.96B/s]\u001b[A\n",
      " 24%|██▍       | 99440640/407873900 [00:14<00:41, 7465814.01B/s]\u001b[A\n",
      " 25%|██▍       | 100199424/407873900 [00:15<00:41, 7457174.92B/s]\u001b[A\n",
      " 25%|██▍       | 100954112/407873900 [00:15<00:44, 6941099.99B/s]\u001b[A\n",
      " 25%|██▍       | 101842944/407873900 [00:15<00:41, 7428857.96B/s]\u001b[A\n",
      " 25%|██▌       | 102699008/407873900 [00:15<00:39, 7703490.31B/s]\u001b[A\n",
      " 25%|██▌       | 103485440/407873900 [00:15<00:42, 7201128.61B/s]\u001b[A\n",
      " 26%|██▌       | 104223744/407873900 [00:15<00:42, 7178237.32B/s]\u001b[A\n",
      " 26%|██▌       | 105058304/407873900 [00:15<00:40, 7416563.87B/s]\u001b[A\n",
      " 26%|██▌       | 105844736/407873900 [00:15<00:40, 7537718.06B/s]\u001b[A\n",
      " 26%|██▌       | 106614784/407873900 [00:15<00:41, 7330804.36B/s]\u001b[A\n",
      " 26%|██▋       | 107355136/407873900 [00:16<00:42, 7093191.41B/s]\u001b[A\n",
      " 27%|██▋       | 108237824/407873900 [00:16<00:39, 7536348.00B/s]\u001b[A\n",
      " 27%|██▋       | 109003776/407873900 [00:16<00:41, 7116445.63B/s]\u001b[A\n",
      " 27%|██▋       | 109793280/407873900 [00:16<00:40, 7285918.83B/s]\u001b[A\n",
      " 27%|██▋       | 110606336/407873900 [00:16<00:39, 7513056.99B/s]\u001b[A\n",
      " 27%|██▋       | 111367168/407873900 [00:16<00:43, 6864845.27B/s]\u001b[A\n",
      " 27%|██▋       | 112071680/407873900 [00:16<00:45, 6471847.30B/s]\u001b[A\n",
      " 28%|██▊       | 112955392/407873900 [00:16<00:42, 6986459.65B/s]\u001b[A\n",
      " 28%|██▊       | 113725440/407873900 [00:16<00:40, 7175876.48B/s]\u001b[A\n",
      " 28%|██▊       | 114460672/407873900 [00:17<00:43, 6761208.88B/s]\u001b[A\n",
      " 28%|██▊       | 115154944/407873900 [00:17<00:45, 6477054.65B/s]\u001b[A\n",
      " 28%|██▊       | 116101120/407873900 [00:17<00:40, 7120045.68B/s]\u001b[A\n",
      " 29%|██▊       | 116843520/407873900 [00:17<00:42, 6845372.89B/s]\u001b[A\n",
      " 29%|██▉       | 117641216/407873900 [00:17<00:42, 6886764.58B/s]\u001b[A\n",
      " 29%|██▉       | 118624256/407873900 [00:17<00:38, 7560701.52B/s]\u001b[A\n",
      " 29%|██▉       | 119411712/407873900 [00:17<00:39, 7370112.81B/s]\u001b[A\n",
      " 29%|██▉       | 120172544/407873900 [00:17<00:40, 7054833.60B/s]\u001b[A\n",
      " 30%|██▉       | 120999936/407873900 [00:17<00:39, 7240587.56B/s]\u001b[A\n",
      " 30%|██▉       | 121802752/407873900 [00:18<00:38, 7446548.46B/s]\u001b[A\n",
      " 30%|███       | 122559488/407873900 [00:18<00:41, 6826761.83B/s]\u001b[A\n",
      " 30%|███       | 123293696/407873900 [00:18<00:41, 6820987.57B/s]\u001b[A\n",
      " 30%|███       | 124276736/407873900 [00:18<00:37, 7496375.09B/s]\u001b[A\n",
      " 31%|███       | 125056000/407873900 [00:18<00:40, 6928588.05B/s]\u001b[A\n",
      " 31%|███       | 125777920/407873900 [00:18<00:40, 6914997.44B/s]\u001b[A\n",
      " 31%|███       | 126652416/407873900 [00:18<00:38, 7239690.40B/s]\u001b[A\n",
      " 31%|███       | 127455232/407873900 [00:18<00:37, 7458000.56B/s]\u001b[A\n",
      " 31%|███▏      | 128216064/407873900 [00:18<00:37, 7413059.10B/s]\u001b[A\n",
      " 32%|███▏      | 128967680/407873900 [00:19<00:39, 6990193.72B/s]\u001b[A\n",
      " 32%|███▏      | 129748992/407873900 [00:19<00:38, 7176776.80B/s]\u001b[A\n",
      " 32%|███▏      | 130477056/407873900 [00:19<01:11, 3884653.80B/s]\u001b[A\n",
      " 32%|███▏      | 131065856/407873900 [00:19<01:04, 4324891.45B/s]\u001b[A\n",
      " 32%|███▏      | 131641344/407873900 [00:19<00:59, 4670959.19B/s]\u001b[A\n",
      " 32%|███▏      | 132272128/407873900 [00:19<00:54, 5065196.39B/s]\u001b[A\n",
      " 33%|███▎      | 133124096/407873900 [00:19<00:48, 5718964.84B/s]\u001b[A\n",
      " 33%|███▎      | 133783552/407873900 [00:20<00:47, 5736627.75B/s]\u001b[A\n",
      " 33%|███▎      | 134491136/407873900 [00:20<00:44, 6081598.05B/s]\u001b[A\n",
      " 33%|███▎      | 135237632/407873900 [00:20<00:43, 6319895.20B/s]\u001b[A\n",
      " 33%|███▎      | 136089600/407873900 [00:20<00:39, 6826970.35B/s]\u001b[A\n",
      " 34%|███▎      | 136809472/407873900 [00:20<00:40, 6730159.35B/s]\u001b[A\n",
      " 34%|███▎      | 137508864/407873900 [00:20<00:46, 5868375.25B/s]\u001b[A\n",
      " 34%|███▍      | 138317824/407873900 [00:20<00:42, 6393165.28B/s]\u001b[A\n",
      " 34%|███▍      | 139071488/407873900 [00:20<00:40, 6606286.06B/s]\u001b[A\n",
      " 34%|███▍      | 139827200/407873900 [00:20<00:39, 6864434.81B/s]\u001b[A\n",
      " 34%|███▍      | 140644352/407873900 [00:21<00:37, 7066954.47B/s]\u001b[A\n",
      " 35%|███▍      | 141430784/407873900 [00:21<00:36, 7275138.00B/s]\u001b[A\n",
      " 35%|███▍      | 142249984/407873900 [00:21<00:38, 6989957.80B/s]\u001b[A\n",
      " 35%|███▌      | 143167488/407873900 [00:21<00:35, 7508112.18B/s]\u001b[A\n",
      " 35%|███▌      | 143938560/407873900 [00:21<00:35, 7537103.07B/s]\u001b[A\n",
      " 35%|███▌      | 144706560/407873900 [00:21<00:38, 6787222.79B/s]\u001b[A\n",
      " 36%|███▌      | 145495040/407873900 [00:21<00:37, 7077521.40B/s]\u001b[A\n",
      " 36%|███▌      | 146223104/407873900 [00:21<00:37, 6970793.30B/s]\u001b[A\n",
      " 36%|███▌      | 146934784/407873900 [00:21<00:41, 6347515.35B/s]\u001b[A\n",
      " 36%|███▌      | 147623936/407873900 [00:22<00:42, 6189209.89B/s]\u001b[A\n",
      " 36%|███▋      | 148656128/407873900 [00:22<00:38, 6742022.06B/s]\u001b[A\n",
      " 37%|███▋      | 149540864/407873900 [00:22<00:35, 7255738.00B/s]\u001b[A\n",
      " 37%|███▋      | 150295552/407873900 [00:22<00:37, 6827391.88B/s]\u001b[A\n",
      " 37%|███▋      | 151293952/407873900 [00:22<00:34, 7539889.23B/s]\u001b[A\n",
      " 37%|███▋      | 152089600/407873900 [00:22<00:34, 7486811.86B/s]\u001b[A\n",
      " 37%|███▋      | 152867840/407873900 [00:22<00:35, 7118434.26B/s]\u001b[A\n",
      " 38%|███▊      | 153653248/407873900 [00:22<00:34, 7303401.98B/s]\u001b[A\n",
      " 38%|███▊      | 154570752/407873900 [00:22<00:32, 7697562.26B/s]\u001b[A\n",
      " 38%|███▊      | 155359232/407873900 [00:23<00:34, 7260522.98B/s]\u001b[A\n",
      " 38%|███▊      | 156103680/407873900 [00:23<00:36, 6991886.66B/s]\u001b[A\n",
      " 38%|███▊      | 156946432/407873900 [00:23<00:34, 7355740.09B/s]\u001b[A\n",
      " 39%|███▊      | 157739008/407873900 [00:23<00:33, 7517714.52B/s]\u001b[A\n",
      " 39%|███▉      | 158502912/407873900 [00:23<00:37, 6712482.17B/s]\u001b[A\n",
      " 39%|███▉      | 159199232/407873900 [00:23<00:38, 6487490.19B/s]\u001b[A\n",
      " 39%|███▉      | 160174080/407873900 [00:23<00:34, 7143792.94B/s]\u001b[A\n",
      " 39%|███▉      | 160944128/407873900 [00:23<00:34, 7239872.65B/s]\u001b[A\n",
      " 40%|███▉      | 161692672/407873900 [00:24<00:36, 6784591.61B/s]\u001b[A\n",
      " 40%|███▉      | 162394112/407873900 [00:24<00:39, 6237605.61B/s]\u001b[A\n",
      " 40%|████      | 163385344/407873900 [00:24<00:35, 6819350.45B/s]\u001b[A\n",
      " 40%|████      | 164237312/407873900 [00:24<00:33, 7241712.15B/s]\u001b[A\n",
      " 40%|████      | 164992000/407873900 [00:24<00:35, 6897460.26B/s]\u001b[A\n",
      " 41%|████      | 165706752/407873900 [00:24<00:35, 6737505.41B/s]\u001b[A\n",
      " 41%|████      | 166547456/407873900 [00:24<00:33, 7100271.10B/s]\u001b[A\n",
      " 41%|████      | 167497728/407873900 [00:24<00:31, 7680435.95B/s]\u001b[A\n",
      " 41%|████▏     | 168292352/407873900 [00:24<00:32, 7374007.92B/s]\u001b[A\n",
      " 41%|████▏     | 169051136/407873900 [00:25<00:32, 7335912.29B/s]\u001b[A\n",
      " 42%|████▏     | 169799680/407873900 [00:25<00:32, 7372592.54B/s]\u001b[A\n",
      " 42%|████▏     | 170708992/407873900 [00:25<00:30, 7811615.57B/s]\u001b[A\n",
      " 42%|████▏     | 171504640/407873900 [00:25<00:32, 7219640.48B/s]\u001b[A\n",
      " 42%|████▏     | 172246016/407873900 [00:25<00:35, 6636535.56B/s]\u001b[A\n",
      " 42%|████▏     | 173068288/407873900 [00:25<00:34, 6738672.60B/s]\u001b[A\n",
      " 43%|████▎     | 174018560/407873900 [00:25<00:31, 7381862.23B/s]\u001b[A\n",
      " 43%|████▎     | 174786560/407873900 [00:25<00:33, 6856469.04B/s]\u001b[A\n",
      " 43%|████▎     | 175552512/407873900 [00:25<00:32, 7078087.72B/s]\u001b[A\n",
      " 43%|████▎     | 176282624/407873900 [00:26<00:32, 7067031.39B/s]\u001b[A\n",
      " 43%|████▎     | 177049600/407873900 [00:26<00:31, 7226509.93B/s]\u001b[A\n",
      " 44%|████▎     | 177803264/407873900 [00:26<00:31, 7303458.93B/s]\u001b[A\n",
      " 44%|████▍     | 178542592/407873900 [00:26<00:31, 7311780.40B/s]\u001b[A\n",
      " 44%|████▍     | 179425280/407873900 [00:26<00:29, 7697190.37B/s]\u001b[A\n",
      " 44%|████▍     | 180204544/407873900 [00:26<00:33, 6845835.17B/s]\u001b[A\n",
      " 44%|████▍     | 181100544/407873900 [00:26<00:30, 7366437.72B/s]\u001b[A\n",
      " 45%|████▍     | 181865472/407873900 [00:26<00:32, 6994703.64B/s]\u001b[A\n",
      " 45%|████▍     | 182734848/407873900 [00:26<00:30, 7420943.98B/s]\u001b[A\n",
      " 45%|████▍     | 183500800/407873900 [00:27<00:30, 7258061.46B/s]\u001b[A\n",
      " 45%|████▌     | 184281088/407873900 [00:27<00:30, 7413304.23B/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 185044992/407873900 [00:27<00:33, 6718764.67B/s]\u001b[A\n",
      " 46%|████▌     | 186077184/407873900 [00:27<00:31, 7014355.99B/s]\u001b[A\n",
      " 46%|████▌     | 187111424/407873900 [00:27<00:28, 7763147.49B/s]\u001b[A\n",
      " 46%|████▌     | 187925504/407873900 [00:27<00:31, 6951206.45B/s]\u001b[A\n",
      " 46%|████▋     | 188663808/407873900 [00:27<00:33, 6448712.76B/s]\u001b[A\n",
      " 46%|████▋     | 189485056/407873900 [00:27<00:31, 6891862.96B/s]\u001b[A\n",
      " 47%|████▋     | 190369792/407873900 [00:28<00:31, 6981451.74B/s]\u001b[A\n",
      " 47%|████▋     | 191177728/407873900 [00:28<00:29, 7278097.87B/s]\u001b[A\n",
      " 47%|████▋     | 191926272/407873900 [00:28<00:29, 7279157.62B/s]\u001b[A\n",
      " 47%|████▋     | 192745472/407873900 [00:28<00:28, 7450486.39B/s]\u001b[A\n",
      " 47%|████▋     | 193502208/407873900 [00:28<00:30, 7133215.76B/s]\u001b[A\n",
      " 48%|████▊     | 194226176/407873900 [00:28<00:30, 6943036.19B/s]\u001b[A\n",
      " 48%|████▊     | 195137536/407873900 [00:28<00:29, 7250933.84B/s]\u001b[A\n",
      " 48%|████▊     | 195872768/407873900 [00:28<00:29, 7223536.15B/s]\u001b[A\n",
      " 48%|████▊     | 196601856/407873900 [00:28<00:32, 6439009.28B/s]\u001b[A\n",
      " 48%|████▊     | 197496832/407873900 [00:28<00:29, 7029269.85B/s]\u001b[A\n",
      " 49%|████▊     | 198231040/407873900 [00:29<00:30, 6951602.66B/s]\u001b[A\n",
      " 49%|████▉     | 198947840/407873900 [00:29<00:34, 6029423.80B/s]\u001b[A\n",
      " 49%|████▉     | 199741440/407873900 [00:29<00:33, 6293254.96B/s]\u001b[A\n",
      " 49%|████▉     | 200773632/407873900 [00:29<00:29, 7026042.09B/s]\u001b[A\n",
      " 49%|████▉     | 201524224/407873900 [00:29<00:29, 7075095.53B/s]\u001b[A\n",
      " 50%|████▉     | 202265600/407873900 [00:29<00:30, 6680627.37B/s]\u001b[A\n",
      " 50%|████▉     | 203149312/407873900 [00:29<00:28, 7073224.42B/s]\u001b[A\n",
      " 50%|█████     | 203952128/407873900 [00:29<00:27, 7306208.74B/s]\u001b[A\n",
      " 50%|█████     | 204702720/407873900 [00:30<00:30, 6639554.05B/s]\u001b[A\n",
      " 50%|█████     | 205392896/407873900 [00:30<00:31, 6475162.98B/s]\u001b[A\n",
      " 51%|█████     | 206344192/407873900 [00:30<00:28, 7088290.47B/s]\u001b[A\n",
      " 51%|█████     | 207084544/407873900 [00:30<00:28, 6953051.95B/s]\u001b[A\n",
      " 51%|█████     | 207900672/407873900 [00:30<00:27, 7175167.71B/s]\u001b[A\n",
      " 51%|█████     | 208752640/407873900 [00:30<00:26, 7522931.70B/s]\u001b[A\n",
      " 51%|█████▏    | 209521664/407873900 [00:30<00:27, 7204168.94B/s]\u001b[A\n",
      " 52%|█████▏    | 210256896/407873900 [00:30<00:30, 6563739.60B/s]\u001b[A\n",
      " 52%|█████▏    | 211062784/407873900 [00:31<00:33, 5904594.22B/s]\u001b[A\n",
      " 52%|█████▏    | 212094976/407873900 [00:31<00:29, 6540061.24B/s]\u001b[A\n",
      " 52%|█████▏    | 212930560/407873900 [00:31<00:27, 6979795.54B/s]\u001b[A\n",
      " 52%|█████▏    | 213665792/407873900 [00:31<00:30, 6440154.95B/s]\u001b[A\n",
      " 53%|█████▎    | 214343680/407873900 [00:31<00:29, 6474673.12B/s]\u001b[A\n",
      " 53%|█████▎    | 215257088/407873900 [00:31<00:27, 7017383.70B/s]\u001b[A\n",
      " 53%|█████▎    | 215989248/407873900 [00:31<00:29, 6554431.89B/s]\u001b[A\n",
      " 53%|█████▎    | 216672256/407873900 [00:31<00:30, 6322880.51B/s]\u001b[A\n",
      " 53%|█████▎    | 217698304/407873900 [00:31<00:26, 7116209.29B/s]\u001b[A\n",
      " 54%|█████▎    | 218458112/407873900 [00:32<00:26, 7206367.40B/s]\u001b[A\n",
      " 54%|█████▎    | 219212800/407873900 [00:32<00:27, 6955614.09B/s]\u001b[A\n",
      " 54%|█████▍    | 219933696/407873900 [00:32<00:27, 6888684.26B/s]\u001b[A\n",
      " 54%|█████▍    | 220893184/407873900 [00:32<00:24, 7523424.17B/s]\u001b[A\n",
      " 54%|█████▍    | 221675520/407873900 [00:32<00:26, 6953918.55B/s]\u001b[A\n",
      " 55%|█████▍    | 222400512/407873900 [00:32<00:26, 6994026.99B/s]\u001b[A\n",
      " 55%|█████▍    | 223285248/407873900 [00:32<00:24, 7461654.88B/s]\u001b[A\n",
      " 55%|█████▍    | 224055296/407873900 [00:32<00:24, 7482071.20B/s]\u001b[A\n",
      " 55%|█████▌    | 224820224/407873900 [00:32<00:25, 7089973.99B/s]\u001b[A\n",
      " 55%|█████▌    | 225693696/407873900 [00:33<00:24, 7340068.45B/s]\u001b[A\n",
      " 56%|█████▌    | 226450432/407873900 [00:33<00:24, 7406728.35B/s]\u001b[A\n",
      " 56%|█████▌    | 227201024/407873900 [00:33<00:24, 7430467.11B/s]\u001b[A\n",
      " 56%|█████▌    | 227971072/407873900 [00:33<00:25, 7168715.02B/s]\u001b[A\n",
      " 56%|█████▌    | 229003264/407873900 [00:33<00:23, 7550118.84B/s]\u001b[A\n",
      " 56%|█████▋    | 229769216/407873900 [00:33<00:23, 7564614.59B/s]\u001b[A\n",
      " 57%|█████▋    | 230533120/407873900 [00:33<00:24, 7356025.20B/s]\u001b[A\n",
      " 57%|█████▋    | 231275520/407873900 [00:33<00:25, 6874232.99B/s]\u001b[A\n",
      " 57%|█████▋    | 232247296/407873900 [00:33<00:24, 7264143.80B/s]\u001b[A\n",
      " 57%|█████▋    | 233064448/407873900 [00:34<00:23, 7514016.00B/s]\u001b[A\n",
      " 57%|█████▋    | 233828352/407873900 [00:34<00:24, 7067632.11B/s]\u001b[A\n",
      " 58%|█████▊    | 234549248/407873900 [00:34<00:25, 6868265.70B/s]\u001b[A\n",
      " 58%|█████▊    | 235425792/407873900 [00:34<00:24, 7120742.92B/s]\u001b[A\n",
      " 58%|█████▊    | 236425216/407873900 [00:34<00:22, 7785997.91B/s]\u001b[A\n",
      " 58%|█████▊    | 237230080/407873900 [00:34<00:24, 6856459.13B/s]\u001b[A\n",
      " 58%|█████▊    | 237954048/407873900 [00:34<00:25, 6734313.50B/s]\u001b[A\n",
      " 59%|█████▊    | 238784512/407873900 [00:34<00:24, 7036552.44B/s]\u001b[A\n",
      " 59%|█████▊    | 239511552/407873900 [00:34<00:23, 7074371.03B/s]\u001b[A\n",
      " 59%|█████▉    | 240235520/407873900 [00:35<00:24, 6806786.60B/s]\u001b[A\n",
      " 59%|█████▉    | 241160192/407873900 [00:35<00:22, 7370356.60B/s]\u001b[A\n",
      " 59%|█████▉    | 242053120/407873900 [00:35<00:21, 7777130.71B/s]\u001b[A\n",
      " 60%|█████▉    | 242852864/407873900 [00:35<00:22, 7257365.73B/s]\u001b[A\n",
      " 60%|█████▉    | 243601408/407873900 [00:35<00:23, 6848282.89B/s]\u001b[A\n",
      " 60%|█████▉    | 244434944/407873900 [00:35<00:22, 7234640.76B/s]\u001b[A\n",
      " 60%|██████    | 245420032/407873900 [00:35<00:20, 7802533.40B/s]\u001b[A\n",
      " 60%|██████    | 246227968/407873900 [00:35<00:21, 7386935.54B/s]\u001b[A\n",
      " 61%|██████    | 246990848/407873900 [00:35<00:23, 6743316.98B/s]\u001b[A\n",
      " 61%|██████    | 247779328/407873900 [00:36<00:24, 6453276.70B/s]\u001b[A\n",
      " 61%|██████    | 248811520/407873900 [00:36<00:22, 6923190.53B/s]\u001b[A\n",
      " 61%|██████    | 249679872/407873900 [00:36<00:21, 7370858.90B/s]\u001b[A\n",
      " 61%|██████▏   | 250442752/407873900 [00:36<00:22, 7065833.81B/s]\u001b[A\n",
      " 62%|██████▏   | 251169792/407873900 [00:36<00:22, 6850425.14B/s]\u001b[A\n",
      " 62%|██████▏   | 251990016/407873900 [00:36<00:22, 7051112.51B/s]\u001b[A\n",
      " 62%|██████▏   | 252841984/407873900 [00:36<00:20, 7399798.60B/s]\u001b[A\n",
      " 62%|██████▏   | 253595648/407873900 [00:36<00:22, 6981835.29B/s]\u001b[A\n",
      " 62%|██████▏   | 254464000/407873900 [00:36<00:20, 7414608.85B/s]\u001b[A\n",
      " 63%|██████▎   | 255318016/407873900 [00:37<00:19, 7719632.32B/s]\u001b[A\n",
      " 63%|██████▎   | 256105472/407873900 [00:37<00:20, 7358876.72B/s]\u001b[A\n",
      " 63%|██████▎   | 256872448/407873900 [00:37<00:20, 7424819.50B/s]\u001b[A\n",
      " 63%|██████▎   | 257626112/407873900 [00:37<00:20, 7455094.52B/s]\u001b[A\n",
      " 63%|██████▎   | 258410496/407873900 [00:37<00:19, 7567195.48B/s]\u001b[A\n",
      " 64%|██████▎   | 259173376/407873900 [00:37<00:20, 7324775.95B/s]\u001b[A\n",
      " 64%|██████▍   | 260067328/407873900 [00:37<00:20, 7219205.59B/s]\u001b[A\n",
      " 64%|██████▍   | 261033984/407873900 [00:37<00:18, 7804135.95B/s]\u001b[A\n",
      " 64%|██████▍   | 261831680/407873900 [00:37<00:19, 7437278.03B/s]\u001b[A\n",
      " 64%|██████▍   | 262591488/407873900 [00:38<00:19, 7286705.27B/s]\u001b[A\n",
      " 65%|██████▍   | 263491584/407873900 [00:38<00:18, 7716850.78B/s]\u001b[A\n",
      " 65%|██████▍   | 264310784/407873900 [00:38<00:18, 7706574.73B/s]\u001b[A\n",
      " 65%|██████▍   | 265097216/407873900 [00:38<00:18, 7731419.24B/s]\u001b[A\n",
      " 65%|██████▌   | 265878528/407873900 [00:38<00:19, 7323244.21B/s]\u001b[A\n",
      " 65%|██████▌   | 266752000/407873900 [00:38<00:18, 7678894.72B/s]\u001b[A\n",
      " 66%|██████▌   | 267554816/407873900 [00:38<00:18, 7465265.98B/s]\u001b[A\n",
      " 66%|██████▌   | 268364800/407873900 [00:38<00:18, 7644902.35B/s]\u001b[A\n",
      " 66%|██████▌   | 269136896/407873900 [00:38<00:19, 7210918.82B/s]\u001b[A\n",
      " 66%|██████▌   | 269997056/407873900 [00:39<00:18, 7578406.02B/s]\u001b[A\n",
      " 66%|██████▋   | 270886912/407873900 [00:39<00:17, 7931323.43B/s]\u001b[A\n",
      " 67%|██████▋   | 271693824/407873900 [00:39<00:20, 6662025.22B/s]\u001b[A\n",
      " 67%|██████▋   | 272404480/407873900 [00:39<00:21, 6266027.29B/s]\u001b[A\n",
      " 67%|██████▋   | 273207296/407873900 [00:39<00:20, 6692904.87B/s]\u001b[A\n",
      " 67%|██████▋   | 273944576/407873900 [00:39<00:19, 6860268.33B/s]\u001b[A\n",
      " 67%|██████▋   | 274655232/407873900 [00:39<00:19, 6721705.21B/s]\u001b[A\n",
      " 68%|██████▊   | 275615744/407873900 [00:39<00:18, 7192041.87B/s]\u001b[A\n",
      " 68%|██████▊   | 276402176/407873900 [00:39<00:17, 7378428.49B/s]\u001b[A\n",
      " 68%|██████▊   | 277156864/407873900 [00:40<00:19, 6626815.20B/s]\u001b[A\n",
      " 68%|██████▊   | 277981184/407873900 [00:40<00:18, 7039979.97B/s]\u001b[A\n",
      " 68%|██████▊   | 278710272/407873900 [00:40<00:19, 6690796.58B/s]\u001b[A\n",
      " 69%|██████▊   | 279401472/407873900 [00:40<00:19, 6629347.15B/s]\u001b[A\n",
      " 69%|██████▊   | 280151040/407873900 [00:40<00:18, 6867444.02B/s]\u001b[A\n",
      " 69%|██████▉   | 281122816/407873900 [00:40<00:16, 7529668.71B/s]\u001b[A\n",
      " 69%|██████▉   | 281904128/407873900 [00:40<00:17, 7150586.56B/s]\u001b[A\n",
      " 69%|██████▉   | 282643456/407873900 [00:40<00:17, 7027714.35B/s]\u001b[A\n",
      " 69%|██████▉   | 283441152/407873900 [00:40<00:17, 7287614.21B/s]\u001b[A\n",
      " 70%|██████▉   | 284332032/407873900 [00:41<00:16, 7707472.55B/s]\u001b[A\n",
      " 70%|██████▉   | 285119488/407873900 [00:41<00:17, 7015312.51B/s]\u001b[A\n",
      " 70%|███████   | 285970432/407873900 [00:41<00:16, 7300282.33B/s]\u001b[A\n",
      " 70%|███████   | 286855168/407873900 [00:41<00:16, 7318090.49B/s]\u001b[A\n",
      " 71%|███████   | 287753216/407873900 [00:41<00:15, 7747602.67B/s]\u001b[A\n",
      " 71%|███████   | 288544768/407873900 [00:41<00:16, 7165541.16B/s]\u001b[A\n",
      " 71%|███████   | 289312768/407873900 [00:41<00:17, 6776453.60B/s]\u001b[A\n",
      " 71%|███████   | 290361344/407873900 [00:41<00:16, 6941077.17B/s]\u001b[A\n",
      " 71%|███████▏  | 291393536/407873900 [00:42<00:15, 7638384.77B/s]\u001b[A\n",
      " 72%|███████▏  | 292189184/407873900 [00:42<00:16, 7130161.92B/s]\u001b[A\n",
      " 72%|███████▏  | 292931584/407873900 [00:42<00:16, 6854363.33B/s]\u001b[A\n",
      " 72%|███████▏  | 293900288/407873900 [00:42<00:15, 7493578.65B/s]\u001b[A\n",
      " 72%|███████▏  | 294709248/407873900 [00:42<00:14, 7662583.09B/s]\u001b[A\n",
      " 72%|███████▏  | 295500800/407873900 [00:42<00:15, 7370123.50B/s]\u001b[A\n",
      " 73%|███████▎  | 296257536/407873900 [00:42<00:15, 6978344.43B/s]\u001b[A\n",
      " 73%|███████▎  | 297095168/407873900 [00:42<00:15, 7312646.21B/s]\u001b[A\n",
      " 73%|███████▎  | 298012672/407873900 [00:42<00:14, 7785837.97B/s]\u001b[A\n",
      " 73%|███████▎  | 298811392/407873900 [00:43<00:14, 7468814.36B/s]\u001b[A\n",
      " 73%|███████▎  | 299575296/407873900 [00:43<00:15, 7130100.91B/s]\u001b[A\n",
      " 74%|███████▎  | 300434432/407873900 [00:43<00:14, 7513279.12B/s]\u001b[A\n",
      " 74%|███████▍  | 301202432/407873900 [00:43<00:14, 7532605.11B/s]\u001b[A\n",
      " 74%|███████▍  | 301967360/407873900 [00:43<00:15, 6984448.91B/s]\u001b[A\n",
      " 74%|███████▍  | 302747648/407873900 [00:43<00:14, 7190927.41B/s]\u001b[A\n",
      " 74%|███████▍  | 303601664/407873900 [00:43<00:13, 7547486.57B/s]\u001b[A\n",
      " 75%|███████▍  | 304370688/407873900 [00:43<00:14, 7020717.86B/s]\u001b[A\n",
      " 75%|███████▍  | 305090560/407873900 [00:43<00:14, 7018512.60B/s]\u001b[A\n",
      " 75%|███████▌  | 306024448/407873900 [00:44<00:13, 7526789.81B/s]\u001b[A\n",
      " 75%|███████▌  | 306796544/407873900 [00:44<00:14, 7144204.01B/s]\u001b[A\n",
      " 75%|███████▌  | 307528704/407873900 [00:44<00:14, 6751778.90B/s]\u001b[A\n",
      " 76%|███████▌  | 308498432/407873900 [00:44<00:13, 7343182.12B/s]\u001b[A\n",
      " 76%|███████▌  | 309366784/407873900 [00:44<00:12, 7689722.12B/s]\u001b[A\n",
      " 76%|███████▌  | 310159360/407873900 [00:44<00:13, 7193131.71B/s]\u001b[A\n",
      " 76%|███████▌  | 310901760/407873900 [00:44<00:13, 6987494.72B/s]\u001b[A\n",
      " 76%|███████▋  | 311693312/407873900 [00:44<00:13, 7219549.38B/s]\u001b[A\n",
      " 77%|███████▋  | 312429568/407873900 [00:44<00:13, 7234933.75B/s]\u001b[A\n",
      " 77%|███████▋  | 313188352/407873900 [00:44<00:12, 7337012.65B/s]\u001b[A\n",
      " 77%|███████▋  | 313954304/407873900 [00:45<00:12, 7279848.36B/s]\u001b[A\n",
      " 77%|███████▋  | 314687488/407873900 [00:45<00:12, 7280442.51B/s]\u001b[A\n",
      " 77%|███████▋  | 315445248/407873900 [00:45<00:13, 6794034.89B/s]\u001b[A\n",
      " 78%|███████▊  | 316493824/407873900 [00:45<00:12, 7044789.56B/s]\u001b[A\n",
      " 78%|███████▊  | 317509632/407873900 [00:45<00:11, 7733567.72B/s]\u001b[A\n",
      " 78%|███████▊  | 318310400/407873900 [00:45<00:12, 7107917.07B/s]\u001b[A\n",
      " 78%|███████▊  | 319049728/407873900 [00:45<00:14, 6313179.23B/s]\u001b[A\n",
      " 78%|███████▊  | 319787008/407873900 [00:45<00:13, 6380782.03B/s]\u001b[A\n",
      " 79%|███████▊  | 320451584/407873900 [00:46<00:13, 6426372.01B/s]\u001b[A\n",
      " 79%|███████▊  | 321130496/407873900 [00:46<00:13, 6529875.54B/s]\u001b[A\n",
      " 79%|███████▉  | 321998848/407873900 [00:46<00:12, 6976433.64B/s]\u001b[A\n",
      " 79%|███████▉  | 322785280/407873900 [00:46<00:11, 7207531.35B/s]\u001b[A\n",
      " 79%|███████▉  | 323520512/407873900 [00:46<00:12, 6902093.94B/s]\u001b[A\n",
      " 80%|███████▉  | 324374528/407873900 [00:46<00:11, 7236563.05B/s]\u001b[A\n",
      " 80%|███████▉  | 325111808/407873900 [00:46<00:11, 7188292.02B/s]\u001b[A\n",
      " 80%|███████▉  | 325840896/407873900 [00:46<00:12, 6700089.31B/s]\u001b[A\n",
      " 80%|████████  | 326799360/407873900 [00:46<00:11, 6925635.67B/s]\u001b[A\n",
      " 80%|████████  | 327778304/407873900 [00:47<00:10, 7591389.04B/s]\u001b[A\n",
      " 81%|████████  | 328564736/407873900 [00:47<00:11, 6968015.71B/s]\u001b[A\n",
      " 81%|████████  | 329290752/407873900 [00:47<00:11, 6764230.67B/s]\u001b[A\n",
      " 81%|████████  | 329994240/407873900 [00:47<00:11, 6836327.79B/s]\u001b[A\n",
      " 81%|████████  | 330693632/407873900 [00:47<00:11, 6472684.05B/s]\u001b[A\n",
      " 81%|████████▏ | 331583488/407873900 [00:47<00:10, 6937934.38B/s]\u001b[A\n",
      " 81%|████████▏ | 332337152/407873900 [00:47<00:10, 7090844.23B/s]\u001b[A\n",
      " 82%|████████▏ | 333061120/407873900 [00:47<00:10, 6835820.09B/s]\u001b[A\n",
      " 82%|████████▏ | 333795328/407873900 [00:47<00:10, 6931870.73B/s]\u001b[A\n",
      " 82%|████████▏ | 334745600/407873900 [00:48<00:09, 7531808.03B/s]\u001b[A\n",
      " 82%|████████▏ | 335520768/407873900 [00:48<00:10, 6831974.25B/s]\u001b[A\n",
      " 82%|████████▏ | 336231424/407873900 [00:48<00:12, 5784262.44B/s]\u001b[A\n",
      " 83%|████████▎ | 337121280/407873900 [00:48<00:11, 6260855.02B/s]\u001b[A\n",
      " 83%|████████▎ | 338120704/407873900 [00:48<00:10, 6789984.05B/s]\u001b[A\n",
      " 83%|████████▎ | 338843648/407873900 [00:48<00:10, 6717641.98B/s]\u001b[A\n",
      " 83%|████████▎ | 339547136/407873900 [00:48<00:10, 6440528.83B/s]\u001b[A\n",
      " 83%|████████▎ | 340299776/407873900 [00:48<00:10, 6722687.33B/s]\u001b[A\n",
      " 84%|████████▎ | 340993024/407873900 [00:49<00:10, 6561964.02B/s]\u001b[A\n",
      " 84%|████████▍ | 341692416/407873900 [00:49<00:10, 6527051.15B/s]\u001b[A\n",
      " 84%|████████▍ | 342724608/407873900 [00:49<00:09, 7013113.61B/s]\u001b[A\n",
      " 84%|████████▍ | 343444480/407873900 [00:49<00:09, 7042148.17B/s]\u001b[A\n",
      " 84%|████████▍ | 344231936/407873900 [00:49<00:08, 7262388.83B/s]\u001b[A\n",
      " 85%|████████▍ | 344969216/407873900 [00:49<00:09, 6745890.31B/s]\u001b[A\n",
      " 85%|████████▍ | 345870336/407873900 [00:49<00:08, 7119447.92B/s]\u001b[A\n",
      " 85%|████████▌ | 346869760/407873900 [00:49<00:07, 7767305.53B/s]\u001b[A\n",
      " 85%|████████▌ | 347675648/407873900 [00:49<00:08, 6838548.62B/s]\u001b[A\n",
      " 85%|████████▌ | 348400640/407873900 [00:50<00:09, 6566770.20B/s]\u001b[A\n",
      " 86%|████████▌ | 349097984/407873900 [00:50<00:08, 6544716.50B/s]\u001b[A\n",
      " 86%|████████▌ | 350113792/407873900 [00:50<00:08, 7205283.96B/s]\u001b[A\n",
      " 86%|████████▌ | 350870528/407873900 [00:50<00:07, 7137558.77B/s]\u001b[A\n",
      " 86%|████████▌ | 351609856/407873900 [00:50<00:08, 6900599.21B/s]\u001b[A\n",
      " 86%|████████▋ | 352505856/407873900 [00:50<00:07, 7331840.26B/s]\u001b[A\n",
      " 87%|████████▋ | 353260544/407873900 [00:50<00:07, 7346890.89B/s]\u001b[A\n",
      " 87%|████████▋ | 354010112/407873900 [00:50<00:08, 6564538.06B/s]\u001b[A\n",
      " 87%|████████▋ | 354979840/407873900 [00:50<00:07, 6710177.07B/s]\u001b[A\n",
      " 87%|████████▋ | 356028416/407873900 [00:51<00:07, 6871050.21B/s]\u001b[A\n",
      " 88%|████████▊ | 357076992/407873900 [00:51<00:07, 7097075.61B/s]\u001b[A\n",
      " 88%|████████▊ | 357945344/407873900 [00:51<00:06, 7501373.40B/s]\u001b[A\n",
      " 88%|████████▊ | 358710272/407873900 [00:51<00:06, 7424627.90B/s]\u001b[A\n",
      " 88%|████████▊ | 359463936/407873900 [00:51<00:07, 6861206.70B/s]\u001b[A\n",
      " 88%|████████▊ | 360353792/407873900 [00:51<00:06, 7219513.57B/s]\u001b[A\n",
      " 89%|████████▊ | 361224192/407873900 [00:51<00:06, 7607869.86B/s]\u001b[A\n",
      " 89%|████████▉ | 362002432/407873900 [00:51<00:06, 7409709.96B/s]\u001b[A\n",
      " 89%|████████▉ | 362757120/407873900 [00:52<00:06, 7118862.90B/s]\u001b[A\n",
      " 89%|████████▉ | 363565056/407873900 [00:52<00:06, 7272406.85B/s]\u001b[A\n",
      " 89%|████████▉ | 364301312/407873900 [00:52<00:06, 7191530.25B/s]\u001b[A\n",
      " 90%|████████▉ | 365124608/407873900 [00:52<00:05, 7474778.17B/s]\u001b[A\n",
      " 90%|████████▉ | 365880320/407873900 [00:52<00:06, 6740113.55B/s]\u001b[A\n",
      " 90%|████████▉ | 366858240/407873900 [00:52<00:05, 7068054.95B/s]\u001b[A\n",
      " 90%|█████████ | 367595520/407873900 [00:52<00:05, 7153007.75B/s]\u001b[A\n",
      " 90%|█████████ | 368414720/407873900 [00:52<00:05, 6727577.89B/s]\u001b[A\n",
      " 91%|█████████ | 369350656/407873900 [00:52<00:05, 7346938.98B/s]\u001b[A\n",
      " 91%|█████████ | 370112512/407873900 [00:53<00:05, 6984075.79B/s]\u001b[A\n",
      " 91%|█████████ | 370833408/407873900 [00:53<00:05, 7029963.45B/s]\u001b[A\n",
      " 91%|█████████ | 371593216/407873900 [00:53<00:05, 7027731.43B/s]\u001b[A\n",
      " 91%|█████████▏| 372588544/407873900 [00:53<00:04, 7707048.12B/s]\u001b[A\n",
      " 92%|█████████▏| 373387264/407873900 [00:53<00:04, 6963984.95B/s]\u001b[A\n",
      " 92%|█████████▏| 374117376/407873900 [00:53<00:05, 6328658.32B/s]\u001b[A\n",
      " 92%|█████████▏| 375132160/407873900 [00:53<00:04, 6879033.99B/s]\u001b[A\n",
      " 92%|█████████▏| 375858176/407873900 [00:53<00:04, 6933794.92B/s]\u001b[A\n",
      " 92%|█████████▏| 376579072/407873900 [00:54<00:04, 6429604.91B/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 377262080/407873900 [00:54<00:04, 6408135.30B/s]\u001b[A\n",
      " 93%|█████████▎| 378277888/407873900 [00:54<00:04, 7043137.41B/s]\u001b[A\n",
      " 93%|█████████▎| 379014144/407873900 [00:54<00:04, 7023187.83B/s]\u001b[A\n",
      " 93%|█████████▎| 379752448/407873900 [00:54<00:04, 6281336.30B/s]\u001b[A\n",
      " 93%|█████████▎| 380801024/407873900 [00:54<00:04, 6403668.20B/s]\u001b[A\n",
      " 94%|█████████▎| 381849600/407873900 [00:54<00:03, 6649872.22B/s]\u001b[A\n",
      " 94%|█████████▍| 382898176/407873900 [00:54<00:03, 7048195.96B/s]\u001b[A\n",
      " 94%|█████████▍| 383930368/407873900 [00:55<00:03, 7416190.27B/s]\u001b[A\n",
      " 94%|█████████▍| 384720896/407873900 [00:55<00:03, 7556315.46B/s]\u001b[A\n",
      " 95%|█████████▍| 385490944/407873900 [00:55<00:03, 7098659.13B/s]\u001b[A\n",
      " 95%|█████████▍| 386240512/407873900 [00:55<00:03, 6796332.28B/s]\u001b[A\n",
      " 95%|█████████▍| 387289088/407873900 [00:55<00:02, 6888112.05B/s]\u001b[A\n",
      " 95%|█████████▌| 388321280/407873900 [00:55<00:02, 7304383.37B/s]\u001b[A\n",
      " 95%|█████████▌| 389287936/407873900 [00:55<00:02, 7858775.68B/s]\u001b[A\n",
      " 96%|█████████▌| 390095872/407873900 [00:55<00:02, 7104889.53B/s]\u001b[A\n",
      " 96%|█████████▌| 390843392/407873900 [00:55<00:02, 7210924.69B/s]\u001b[A\n",
      " 96%|█████████▌| 391731200/407873900 [00:56<00:02, 7640788.77B/s]\u001b[A\n",
      " 96%|█████████▌| 392516608/407873900 [00:56<00:02, 7451696.14B/s]\u001b[A\n",
      " 96%|█████████▋| 393278464/407873900 [00:56<00:02, 6810991.27B/s]\u001b[A\n",
      " 97%|█████████▋| 394219520/407873900 [00:56<00:01, 7097992.96B/s]\u001b[A\n",
      " 97%|█████████▋| 395130880/407873900 [00:56<00:01, 7602042.79B/s]\u001b[A\n",
      " 97%|█████████▋| 395914240/407873900 [00:56<00:01, 7093789.70B/s]\u001b[A\n",
      " 97%|█████████▋| 396647424/407873900 [00:56<00:01, 6753632.73B/s]\u001b[A\n",
      " 97%|█████████▋| 397398016/407873900 [00:56<00:01, 6849387.76B/s]\u001b[A\n",
      " 98%|█████████▊| 398399488/407873900 [00:57<00:01, 7565816.98B/s]\u001b[A\n",
      " 98%|█████████▊| 399188992/407873900 [00:57<00:01, 7406301.83B/s]\u001b[A\n",
      " 98%|█████████▊| 399953920/407873900 [00:57<00:01, 7129160.25B/s]\u001b[A\n",
      " 98%|█████████▊| 400740352/407873900 [00:57<00:00, 7315030.10B/s]\u001b[A\n",
      " 98%|█████████▊| 401690624/407873900 [00:57<00:00, 7762331.74B/s]\u001b[A\n",
      " 99%|█████████▊| 402485248/407873900 [00:57<00:00, 7058930.76B/s]\u001b[A\n",
      " 99%|█████████▉| 403216384/407873900 [00:57<00:00, 6657084.01B/s]\u001b[A\n",
      " 99%|█████████▉| 403968000/407873900 [00:57<00:00, 6664872.28B/s]\u001b[A\n",
      " 99%|█████████▉| 404967424/407873900 [00:57<00:00, 7398546.50B/s]\u001b[A\n",
      " 99%|█████████▉| 405743616/407873900 [00:58<00:00, 7111241.68B/s]\u001b[A\n",
      "100%|█████████▉| 406482944/407873900 [00:58<00:00, 6664260.89B/s]\u001b[A\n",
      "100%|█████████▉| 407332864/407873900 [00:58<00:00, 7125623.67B/s]\u001b[A\n",
      "100%|██████████| 407873900/407873900 [00:58<00:00, 6991514.55B/s]\u001b[A"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(BERT_MODEL, num_labels = len(label2idx))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "NUM_TRAIN_EPOCHS = 3\n",
    "LEARNING_RATE = 5e-5\n",
    "WARMUP_PROPORTION = 0.1\n",
    "\n",
    "num_train_steps = int(len(train_data) / TRAIN_BATCH_SIZE / GRADIENT_ACCUMULATION_STEPS * NUM_TRAIN_EPOCHS)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "t_total = num_train_steps\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     LEARNING_RATE,\n",
    "                     warmup=WARMUP_PROPORTION,\n",
    "                     t_total=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        \n",
    "\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "    \n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens = tokenizer.tokenize(example.text)\n",
    "\n",
    "        if len(tokens) > max_seq_length - 2:\n",
    "            tokens = tokens[:(max_seq_length - 2)]\n",
    "            \n",
    "        tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "            \n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        label_id = label_map[example.label]\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_id))\n",
    "    return features\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/27/2018 21:13:12 - INFO - __main__ -   *** Example ***\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   tokens: [CLS] from : le ##r ##x ##st @ wa ##m . um ##d . ed ##u ( where ' [SEP]\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   input_ids: 101 2013 1024 3393 2099 2595 3367 1030 11333 2213 1012 8529 2094 1012 3968 2226 1006 2073 1005 102\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   label: rec.autos (id = 7)\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   *** Example ***\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   tokens: [CLS] from : guy ##ku ##o @ carson . u . washington . ed ##u ( guy ku ##o [SEP]\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   input_ids: 101 2013 1024 3124 5283 2080 1030 9806 1012 1057 1012 2899 1012 3968 2226 1006 3124 13970 2080 102\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   label: comp.sys.mac.hardware (id = 4)\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   *** Example ***\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   tokens: [CLS] from : t ##wil ##lis @ ec . ec ##n . purdue . ed ##u ( thomas e [SEP]\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   input_ids: 101 2013 1024 1056 29602 6856 1030 14925 1012 14925 2078 1012 19749 1012 3968 2226 1006 2726 1041 102\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   label: comp.sys.mac.hardware (id = 4)\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   *** Example ***\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   tokens: [CLS] from : j ##gree ##n @ amber ( joe green ) subject : re : wei ##tek p [SEP]\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   input_ids: 101 2013 1024 1046 28637 2078 1030 8994 1006 3533 2665 1007 3395 1024 2128 1024 11417 23125 1052 102\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   label: comp.graphics (id = 1)\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   *** Example ***\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   tokens: [CLS] from : jc ##m @ head - cfa . harvard . ed ##u ( jonathan mcdowell ) subject [SEP]\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   input_ids: 101 2013 1024 29175 2213 1030 2132 1011 28125 1012 5765 1012 3968 2226 1006 5655 25005 1007 3395 102\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "12/27/2018 21:13:12 - INFO - __main__ -   label: sci.space (id = 14)\n"
     ]
    }
   ],
   "source": [
    "train_features = convert_examples_to_features(train_data, target_names, 20, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/27/2018 21:14:31 - INFO - __main__ -   ***** Running training *****\n",
      "12/27/2018 21:14:31 - INFO - __main__ -     Num examples = 11314\n",
      "12/27/2018 21:14:31 - INFO - __main__ -     Batch size = 32\n",
      "12/27/2018 21:14:31 - INFO - __main__ -     Num steps = 1060\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'TensorDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-d4d1e72ce60e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mall_segment_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegment_ids\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mall_label_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_id\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_input_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_segment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_label_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtrain_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TensorDataset' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", len(train_data))\n",
    "logger.info(\"  Batch size = %d\", TRAIN_BATCH_SIZE)\n",
    "logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "\n",
    "model.train()\n",
    "for _ in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, input_mask, segment_ids, label_ids = batch\n",
    "        loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "        if n_gpu > 1:\n",
    "            loss = loss.mean() # mean() to average on multi-gpu.\n",
    "        if args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "        if args.fp16:\n",
    "            optimizer.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "            # modify learning rate with special warm up BERT uses\n",
    "            lr_this_step = args.learning_rate * warmup_linear(global_step/t_total, args.warmup_proportion)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_this_step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "# Save a trained model\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "output_model_file = os.path.join(args.output_dir, \"pytorch_model.bin\")\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "\n",
    "# Load a trained model that you have fine-tuned\n",
    "model_state_dict = torch.load(output_model_file)\n",
    "model = BertForSequenceClassification.from_pretrained(args.bert_model, state_dict=model_state_dict)\n",
    "model.to(device)\n",
    "\n",
    "eval_examples = processor.get_dev_examples(args.data_dir)\n",
    "eval_features = convert_examples_to_features(\n",
    "    eval_examples, label_list, args.max_seq_length, tokenizer)\n",
    "logger.info(\"***** Running evaluation *****\")\n",
    "logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "# Run prediction for full data\n",
    "eval_sampler = SequentialSampler(eval_data)\n",
    "eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "model.eval()\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "        logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = label_ids.to('cpu').numpy()\n",
    "    tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "\n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    nb_eval_examples += input_ids.size(0)\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "\n",
    "result = {'eval_loss': eval_loss,\n",
    "          'eval_accuracy': eval_accuracy,\n",
    "          'global_step': global_step,\n",
    "          'loss': tr_loss/nb_tr_steps}\n",
    "\n",
    "output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "    logger.info(\"***** Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
